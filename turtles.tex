\documentclass{article}[letter,10pt]
\usepackage{biblatex}[encoding=utf8,backend=biber]

\addbibresource{turtles.bib}

\title{Release the TURTLES\\
  \large{A Pure Tcl Interface to Dynamic Proc Tracing}}
\author{
  Michael Yantosca\\
  FlightAware \\
  michael.yantosca@flightaware.com
}

\begin{document}

\maketitle

\begin{abstract}
  Proper dynamic program analysis requires solid collection of call frequency,
  relationships, and timing. Typically, this is achieved with a suite of
  language-centric tools, e.g., valgrind (C/C++), the GHC profiling subsystem
  (Haskell). Tcl appears to lack similar tools or at least requires significant
  language extensions to meet this need. Consequently, this work introduces
  the Tcl Universal Recursive Trace Log Execution Scrutinizer (TURTLES), a
  simple proc tracing interface that yields easily aggregated and analyzed
  call records. By adhering to pure Tcl, the endeavor provides an enlightening
  étude in the pain points of developing such tooling in the language.
\end{abstract}

\tableofcontents

\section{Introduction}{
  \paragraph{}{
    The TURTLES project began as a small task meant to investigate the
    relationship between different procs within the Tcl repositories comprising
    the FlightAware codebase, particularly for the Multi-Machine HyperFeed (MMHF)
    project. The results of the analysis would be leveraged to inform
    refactoring efforts to improve the performance of the target program.
    Inspired by callgrind\autocite{callgrind} and the GHC profiling
    subsystem\autocite{ghcprof}, the TURTLES project aimed to capture not
    only the call-graph relationships but also performance metrics so that
    users of the library could consolidate correlated calls into logical
    modules as well as pinpoint execution hotspots.
  }
  \paragraph{}{
    Some initial investigation was made into extant facilities within
    the Tcl base or community packages that might achieve this purpose.
    Shannon Noe suggested cmdtrace\autocite{tcl::cmdtrace} and
    disassemble\autocite{tcl::disassemble} as potential candidates.
    After reviewing the documentation, the disassembler appeared to
    be worth investigating as a tool for static analysis in future work.
    To determine the suitability of cmdtrace for dynamic analysis, some dry
    runs were executed on toy programs with cmdtrace turned on. The resulting
    output was verbose and would require extra parsing to convert into
    a usable format. The parsing work was not necessarily prohibitive, but
    complex programs like MMHF could potentially generate unmanageable amounts
    of data, and so it was deemed that a more space-sensitive approach was
    required.  To this end, constraints were placed on the scope of the TURTLES
    project to only examine the immediate caller/callee relationship and provide
    execution timings inclusive of the total time spent both in the callee and
    in the callee's subcalls.
  }
  \paragraph{}{
    The TURTLES project also served as an introductory étude in Tcl introspection.
    As a neophyte in the language, working on the TURTLES project offered an
    accelerated course in namespace management and execution semantics. Pursuing
    a pure Tcl approach forced careful consideration of the costs incurred by
    the profiling overhead and exposed some pitfalls in Tcl that might
    be improved in future revisions of the language to make it more accessible
    and productive for both novice and experienced developers.
  }
  \paragraph{}{
    In order to maximize portability, one tacit goal of the TURTLES is to have
    minimal dependencies on other packages. The list of required packages is
    given as follows:
    \begin{itemize}
    \item{Tcl 8.5\autocite{tcl::85} or 8.6\autocite{tcl::86}}
    \item{Tclx\autocite{tcl::Tclx}}
    \item{Thread\autocite{tcl::Thread}}
    \item{cmdline\autocite{tcl::cmdline}}
    \item{platform\autocite{tcl::platform}}
    \item{sqlite3\autocite{tcl::sqlite3}}
    \item{tcltest\autocite{tcl::tcltest}}
    \end{itemize}
    Building the project requires GNU make or a compatible make utility.
    Code documentation can be generated by doxygen, but installation does
    not require it. The project repository README provides instructions
    on building and installing the TURTLES project, as well as instructions
    for use within a given user-defined program. Some aspects of usage
    will also be covered in the Design and Implementation sections of
    this report.
  }
  \paragraph{}{
    The remainder of this report is organized in the following manner.
    The Design section covers the abstract design decisions and data flow
    for the TURTLES project. The Implementation section expounds on the
    Design section with specific details about how the design was realized.
    The Experiments section exhibits the results of employing the TURTLES
    project on both toy examples and on the larger MMHF project.
    In the Conclusions section, the experimental results are analyzed
    to evaluate the performance and output of the TURTLES project.
    Finally, in the Future Work section, some avenues for ongoing research
    are considered in light of the aforementioned conclusions with the goal of
    improving the TURTLES project so as to encourage adoption and enhance
    the set of available tools for Tcl development.
  }
}

\section{Design}{
  A few guiding principles informed the design of the TURTLES project, namely
  minimization of overhead, correctness of collection, ease of use, and legibility
  of results. For simplicity, the call records do not attempt to store information
  for complete traces through the full call stack but rather capture the immediate
  caller-callee relationship.

  Starting and stopping data collection are each achieved by designated procs.

  \subsubsection{Initialization}{
    Starting data collection is achieved by a designated proc which sets up
    the necessary ephemeral and final storage for call records per arguments
    supplied by the user. A trace handler is added to the `proc` command on exit
    to bootstrap the assignment of trace handlers for entry and exit to every
    proc defined thereafter.

    For best results, it is recommended to make the initialization call as early
    as possible to capture the greatest number of proc definitions. Currently,
    the TURTLES project does not support retroactively adding proc handlers
    to procs already defined.

  }
  \subsubsection{Collection}{
    The entry and exit handlers capture the times of entry and exit into and out of
    the covered proc and record this in the form of a call point record. The caller
    and callee information in each call point record is normalized out to a collection
    of unique proc definition records.

    \paragraph{Proc Definition}{
      Each proc defined after data collection starts is stored in a proc definition
      record. This record is a triple of the proc name, a unique integral identifier,
      and the time of definition. The proc name is the unaliased, fully-qualified
      name of the proc, and the integral identifier is a hash of this value.
      The time of definition is canonically represented in UNIX epoch microseconds.
    }
    \paragraph{Call Point}{
      Each call made is recorded in the form of a call point record. This record
      is a quintuple consisting of a caller ID, callee ID, trace ID, time of
      entry, and time of exit. The caller and callee IDs correspond to unique
      integral identifiers in the set of proc definition records. The trace ID is a
      unique identifier for distinguishing separate calls with the same caller
      and callee and is calculated deterministically. Time of entry and exit are
      canonically represented in UNIX epoch microseconds.
    }
    \paragraph{Persistence}{
      Persistence of call records is handled either directly or in a staged fashion
      depending on the arguments supplied by the user. In direct mode, the call
      records are captured and stored immediately in the final storage. This has
      the benefit of retaining the most information in case execution is interrupted
      but at the cost of speed. In staged mode, the call records are captured and
      stored in ephemeral storage while a background task or thread periodically
      transfers unfinalized records in bulk to the final persistent storage. The
      immediate processing cost of each call record is reduced, but the risk of
      greater overall loss in cases of interruption is increased.
    }
  }
  \subsubsection{Finalization}{
    Ending data collection is achieved by a designated proc which disables
    all the relevant trace handlers along with the handler on the `proc` command.
    If collection was operating in staged mode, the ephemeral storage is flushed
    to the final storage.
  }
  \subsubsection{Analysis}{
    For post-hoc analysis, a clustering tool is provided to construct a call graph
    whose edges are the immediate caller-callee pairings and nodes are individual
    procs. The edge weights are defined as the number of invocations of the callee
    by the caller.

    In anticipation of a highly-connected graph with many nodes, initial attempts
    to realize the clustering tool employed the Gallagher-Humblet-Spira (GHS) distributed
    minimum spanning tree algorithm\autocite[102-106]{DNA}. Development of a $k$-machine
    model\autocite[129-133]{DNA} using Tcl threads as the individual machines was started but ultimately
    abandoned due to low development velocity. In its place, a simple breadth-first
    flooding approach\autocite[55-58]{DNA} was adopted and found to be sufficient to the task.

    The call records themselves are stored in a format that is searchable and aggregable
    via standard SQL statements. This provides the developer with an expressive foundation
    for data analysis and visualization which has long enjoyed wide adoption with a
    minimal learning curve.
  }
}

\section{Implementation}{
  TBD
}

\section{Experiments}{
  TBD
}

\section{Conclusions}{
  TBD
}

\section{Future Work}{
  TBD
}

\printbibliography

\end{document}
